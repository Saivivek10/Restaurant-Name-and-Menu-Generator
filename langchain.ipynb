{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "import openai\n",
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_key = os.getenv('OPENAI_API_KEY')   \n",
    "my_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of France is Paris. \n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0.9, openai_api_key=my_key)\n",
    "text = \"What is the capital of France?\"\n",
    "response = llm(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to open a restaturant for Indian food. suggest a fency name for this'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables= [\"cuisine\"],\n",
    "    template=\"I want to open a restaturant for {cuisine} food. suggest a fency name for this\"\n",
    ")\n",
    "\n",
    "prompt_template_name.format(cuisine=\"Indian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Saffron Spice House\"'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "chain.run(\"Indian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables= [\"cuisine\"],\n",
    "    template= \" I want to open a restaturant for {cuisine} food. suggest a fency name for this\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables= ['restaurant_name'],\n",
    "    template= \"\"\" suggest some popular dishes for a restaturant named {restaurant_name}\". Return it as a comma seperated list\"\"\"\n",
    ")\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Butter Chicken\n",
      "2. Biryani\n",
      "3. Tandoori Chicken\n",
      "4. Lamb Vindaloo\n",
      "5. Chicken Tikka Masala\n",
      "6. Palak Paneer\n",
      "7. Naan Bread\n",
      "8. Samosas\n",
      "9. Chana Masala\n",
      "10. Garlic Naan\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SimpleSequentialChain(chains=[name_chain, food_items_chain])\n",
    "response = chain.run(\"Indian\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables= [\"cuisine\"],\n",
    "    template= \" I want to open a restaturant for {cuisine} food. suggest a fency name for this\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables= ['restaurant_name'],\n",
    "    template= \"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Indian',\n",
       " 'restaurant_name': '\\n\\n\"Mehak-E-India\" (meaning \"Fragrance of India\")',\n",
       " 'menu_items': ' \\n\\nAppetizers: \\n1. Samosas (triangular pastry filled with spiced potatoes and peas)\\n2. Pakoras (deep-fried batter coated vegetables, such as onions or cauliflower)\\n3. Chicken Tikka (grilled marinated chicken skewers)\\n4. Vegetable Samosas (triangular pastry filled with spiced mixed vegetables)\\n5. Tandoori Shrimp (marinated and grilled shrimp)\\n6. Papdi Chaat (crispy fried dough topped with chickpeas, yogurt, and chutney)\\n7. Aloo Tikki (potato patties served with chutney)\\n8. Seekh Kebabs (minced meat skewers)\\n9. Masala Prawns (spicy marinated and grilled prawns)\\n10. Dahi Vada (lentil dumplings served in yogurt with chutney)\\n\\nEntrees:\\n1. Butter Chicken (creamy tomato-based chicken dish)\\n2. Palak Paneer (spinach and cottage cheese curry)\\n3. Biryani (fragrant rice dish with meat or vegetables)\\n4. Chicken Curry (spicy curry made with chicken and traditional Indian spices)\\n5. Paneer Tikka Masala (grilled cottage cheese in a rich tomato and cream'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains=[name_chain, food_items_chain],\n",
    "    input_variables=['cuisine'],\n",
    "    output_variables=['restaurant_name', 'menu_items'],\n",
    "    \n",
    ")\n",
    "\n",
    "chain({'cuisine': 'Indian'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in ./venv2305/lib/python3.13/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv2305/lib/python3.13/site-packages (from wikipedia) (4.13.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in ./venv2305/lib/python3.13/site-packages (from wikipedia) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv2305/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv2305/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv2305/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv2305/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv2305/lib/python3.13/site-packages (from beautifulsoup4->wikipedia) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./venv2305/lib/python3.13/site-packages (from beautifulsoup4->wikipedia) (4.14.1)\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.11.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in ./venv2305/lib/python3.13/site-packages (from numexpr) (2.3.2)\n",
      "Downloading numexpr-2.11.0-cp313-cp313-macosx_11_0_arm64.whl (136 kB)\n",
      "Installing collected packages: numexpr\n",
      "Successfully installed numexpr-2.11.0\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia\n",
    "%pip install numexpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use the calculator for the second question.\n",
      "Action: Calculator\n",
      "Action Input: 2+2\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 4\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use Wikipedia for the first question.\n",
      "Action: wikipedia\n",
      "Action Input: capital of France\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: List of capitals of France\n",
      "Summary: This is a chronological list of capitals of France. The capital of France has been Paris since its liberation in 1944.\n",
      "\n",
      "Page: Capital punishment in France\n",
      "Summary: Capital punishment in France (French: peine de mort en France) is banned by Article 66-1 of the Constitution of the French Republic, voted as a constitutional amendment by the Congress of the French Parliament on 19 February 2007 and simply stating \"No one can be sentenced to the death penalty\" (French: Nul ne peut être condamné à la peine de mort). The death penalty was already declared illegal on 9 October 1981 when President François Mitterrand signed a law prohibiting the judicial system from using it and commuting the sentences of the seven people on death row to life imprisonment. The last execution took place by guillotine, being the main legal method since the French Revolution; Hamida Djandoubi, a Tunisian citizen convicted of torture and murder on French soil, was put to death in September 1977 in Marseille.\n",
      "Major French death penalty abolitionists across time have included philosopher Voltaire; poet Victor Hugo; politicians Léon Gambetta, Jean Jaurès and Aristide Briand; and writers Alphonse de Lamartine and Albert Camus.\n",
      "\n",
      "Page: Capital city\n",
      "Summary: A capital city, or just capital, is the municipality holding primary status in a country, state, province, department, or other subnational division, usually as its seat of the government. A capital is typically a city that physically encompasses the government's offices and meeting places; the status as capital is often designated by its law or constitution. In some jurisdictions, including several countries, different branches of government are in different settlements, sometimes meaning multiple official capitals. In some cases, a distinction is made between the official (constitutional) capital and the seat of government, which is in another place.\n",
      "English-language media often use the name of the capital metonymically to refer to the government sitting there. Thus, \"London-Washington relations\" is widely understood to mean diplomatic relations between Great Britain and the United States.\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The capital of France is Paris and 2+2 is 4.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris and 2+2 is 4.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "tools  = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "\n",
    ")\n",
    "agent.run(\"What is the capital of France? What is 2+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Maharaja's Kitchen\n",
      "2. Spice Kingdom\n",
      "3. Tandoori Temptations\n",
      "4. Curry Club\n",
      "5. Biryani Bliss\n",
      "6. Naan Nirvana\n",
      "7. Taj Mahal Bites\n",
      "8. Masala Magic\n",
      "9. Chai Chaat\n",
      "10. Bollywood Bites\n",
      "11. Saffron Splendor\n",
      "12. The Great Indian Platter\n",
      "13. Namaste Nibbles\n",
      "14. Dhaba Delights\n",
      "15. Desi Cuisine Co.\n",
      "16. Royal Rajasthani\n",
      "17. Delhi Darbar\n",
      "18. Samosa Sizzlers\n",
      "19. Chaat House\n",
      "20. Mughal Mansion.\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "name = chain.run(\"Indian\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"El Dorado Cantina\" \n"
     ]
    }
   ],
   "source": [
    "name= chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InputType',\n",
       " 'OutputType',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__firstlineno__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_computed_fields__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_setattr_handlers__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__replace__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_recursion__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__static_attributes__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abatch_with_config',\n",
       " '_abc_impl',\n",
       " '_acall',\n",
       " '_acall_with_config',\n",
       " '_atransform_stream_with_config',\n",
       " '_batch_with_config',\n",
       " '_calculate_keys',\n",
       " '_call',\n",
       " '_call_with_config',\n",
       " '_chain_type',\n",
       " '_copy_and_set_values',\n",
       " '_get_num_tokens',\n",
       " '_get_value',\n",
       " '_iter',\n",
       " '_parse_generation',\n",
       " '_run_output_key',\n",
       " '_setattr_handler',\n",
       " '_transform_stream_with_config',\n",
       " '_validate_inputs',\n",
       " '_validate_outputs',\n",
       " 'aapply',\n",
       " 'aapply_and_parse',\n",
       " 'abatch',\n",
       " 'abatch_as_completed',\n",
       " 'acall',\n",
       " 'agenerate',\n",
       " 'ainvoke',\n",
       " 'apply',\n",
       " 'apply_and_parse',\n",
       " 'apredict',\n",
       " 'apredict_and_parse',\n",
       " 'aprep_inputs',\n",
       " 'aprep_outputs',\n",
       " 'aprep_prompts',\n",
       " 'arun',\n",
       " 'as_tool',\n",
       " 'assign',\n",
       " 'astream',\n",
       " 'astream_events',\n",
       " 'astream_log',\n",
       " 'atransform',\n",
       " 'batch',\n",
       " 'batch_as_completed',\n",
       " 'bind',\n",
       " 'callback_manager',\n",
       " 'callbacks',\n",
       " 'config_schema',\n",
       " 'config_specs',\n",
       " 'configurable_alternatives',\n",
       " 'configurable_fields',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'create_outputs',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'from_string',\n",
       " 'generate',\n",
       " 'get_config_jsonschema',\n",
       " 'get_graph',\n",
       " 'get_input_jsonschema',\n",
       " 'get_input_schema',\n",
       " 'get_lc_namespace',\n",
       " 'get_name',\n",
       " 'get_output_jsonschema',\n",
       " 'get_output_schema',\n",
       " 'get_prompts',\n",
       " 'input_keys',\n",
       " 'input_schema',\n",
       " 'invoke',\n",
       " 'is_lc_serializable',\n",
       " 'json',\n",
       " 'lc_attributes',\n",
       " 'lc_id',\n",
       " 'lc_secrets',\n",
       " 'llm',\n",
       " 'llm_kwargs',\n",
       " 'map',\n",
       " 'memory',\n",
       " 'metadata',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'name',\n",
       " 'output_key',\n",
       " 'output_keys',\n",
       " 'output_parser',\n",
       " 'output_schema',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'pick',\n",
       " 'pipe',\n",
       " 'predict',\n",
       " 'predict_and_parse',\n",
       " 'prep_inputs',\n",
       " 'prep_outputs',\n",
       " 'prep_prompts',\n",
       " 'prompt',\n",
       " 'raise_callback_manager_deprecation',\n",
       " 'return_final_only',\n",
       " 'run',\n",
       " 'save',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'set_verbose',\n",
       " 'stream',\n",
       " 'tags',\n",
       " 'to_json',\n",
       " 'to_json_not_implemented',\n",
       " 'transform',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'verbose',\n",
       " 'with_alisteners',\n",
       " 'with_config',\n",
       " 'with_fallbacks',\n",
       " 'with_listeners',\n",
       " 'with_retry',\n",
       " 'with_types']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bf/vckk_13516l5hz4wnymdl6k80000gn/T/ipykernel_14986/2645069426.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Spice Mahal\" \n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "name = chain.run(\"Indian\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"El Fuego Fino\"\n"
     ]
    }
   ],
   "source": [
    "name= chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Indian', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n\\n\"Spice Mahal\" ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Mexican', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n\\n\"El Sabroso Cocina\"', additional_kwargs={}, response_metadata={}), HumanMessage(content='Mexican', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n\\n\"El Sabroso Cantina\"', additional_kwargs={}, response_metadata={}), HumanMessage(content='Mexican', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n\\n\"El Fuego Fino\"', additional_kwargs={}, response_metadata={})]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Indian\n",
      "AI: \n",
      "\n",
      "\"Spice Mahal\" \n",
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "\"El Sabroso Cocina\"\n",
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "\"El Sabroso Cantina\"\n",
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "\"El Fuego Fino\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.9), memory=memory)\n",
    "\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The Deccan Chargers won the IPL 2009.\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won IPL 2009?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  14.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"What is 10+4?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     The captain of the Indian cricket team in 2019 was Virat Kohli.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the Indian cricket team in 2019?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        The captain of the winning team in IPL 2009 was Adam Gilchrist from the Deccan Chargers.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team in IPL 2009?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Indian\n",
      "AI: \n",
      "\n",
      "\"Spice Mahal\" \n",
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "\"El Sabroso Cocina\"\n",
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "\"El Sabroso Cantina\"\n",
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "\"El Fuego Fino\"\n",
      "Human: Who won IPL 2025?\n",
      "AI:  I am an AI, so I do not have the ability to predict the future. However, the IPL 2025 has not happened yet, so there is no current answer to your question.\n",
      "Human: Who won IPL 2008?\n",
      "AI:  The Rajasthan Royals won the IPL 2008.\n",
      "Human: Who won IPL 2008?\n",
      "AI:  The Rajasthan Royals won the IPL 2008.\n",
      "Human: Who won IPL 2025?\n",
      "AI:   I am an AI, so I do not have the ability to predict the future. However, the IPL 2025 has not happened yet, so there is no current answer to your question.\n",
      "Human: Who won IPL 2009?\n",
      "AI:  The Deccan Chargers won the IPL 2009.\n",
      "\n",
      "Human: What is the last update of GPT-4?\n",
      "AI:  As of now, there is no official release date for GPT-4. However, there have been speculations that it may be released in 2021 or 2022. But, the development of such an advanced AI system takes time and can be delayed, so there is no exact answer to your question.\n",
      "Human: What is 10+4?\n",
      "AI:   14.\n",
      "Human: Who was the captain of the Indian cricket team in 2007?\n",
      "AI:   The captain of the Indian cricket team in 2007 was Rahul Dravid.\n",
      "Human: Who was the captain of the Indian cricket team in 2012?\n",
      "AI:  The captain of the Indian cricket team in 2012 was Mahendra Singh Dhoni.\n",
      "Human: Who was the captain of the Indian cricket team in 2019?\n",
      "AI:   The captain of the Indian cricket team in 2019 was Virat Kohli.\n",
      "Human: Who was the captain of the Indian cricket team in 2023?\n",
      "AI:    I am sorry, but I do not have information about the future. The captain of the Indian cricket team in 2023 has not been decided yet. \n",
      "Human: Who was the captain of the Indian cricket team in 2022?\n",
      "AI:    As of now, there is no official decision on who will be the captain of the Indian cricket team in 2022. It will depend on various factors such as performance, availability, and team dynamics in the future.\n",
      "Human: Who was the captain of the Indian cricket team in 2021?\n",
      "AI:     Currently, the captain of the Indian cricket team in 2021 is Virat Kohli.\n",
      "Human: Who was the captain of the Indian cricket team in 2020?\n",
      "AI:     The captain of the Indian cricket team in 2020 was Virat Kohli, and he is still the captain in 2021.\n",
      "Human: Who was the captain of the Indian cricket team in 2019?\n",
      "AI:      The captain of the Indian cricket team in 2019 was Virat Kohli.\n",
      "Human: Who was the captain of the winning team in IPL 2009?\n",
      "AI:       The captain of the winning team in IPL 2009 was Adam Gilchrist from the Deccan Chargers.\n",
      "Human: Who was the captain of the winning team?\n",
      "AI:        I am not sure which winning team you are referring to. Could you provide more context?\n",
      "Human: Who was the captain of the winning team in IPL 2009?\n",
      "AI:         The captain of the winning team in IPL 2009 was Adam Gilchrist from the Deccan Chargers.\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The IPL 2009 was won by the Deccan Chargers. They defeated the Royal Challengers Bangalore in the finals by six runs. This was their first and only IPL title.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.9), memory=memory)\n",
    "convo.run(\"Who won IPL 2009?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  The answer to 10+4 is 14.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"What is 10+4?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, I do not have enough context to answer that question. The Indian Premier League (IPL) is a professional Twenty20 cricket league in India. It was established in 2008 and the 2009 season was the second season of the league. The winning team that year was the Deccan Chargers, and their captain was Adam Gilchrist. Is there any other information you would like me to provide?\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team in IPL 2009?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
